{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "85abb822-7eb1-4e2d-ada8-be033f3d1c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ff73813-dbd2-4996-8d6a-eed1ba48b441",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../data/heart_disease_clean.csv\")\n",
    "X = df.drop(columns=['target'])\n",
    "y = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63709224-bd2a-4f0e-ac52-893ebc7c0195",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42),\n",
    "    \"SVM\": SVC(probability=True, random_state=42)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e24bfa63-4140-4edf-837c-af68f006f41b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 Model  Accuracy  Precision    Recall        F1   ROC_AUC\n",
      "0  Logistic Regression  0.833333   0.846154  0.785714  0.814815  0.949777\n",
      "1        Decision Tree  0.683333   0.680000  0.607143  0.641509  0.678571\n",
      "2        Random Forest  0.866667   0.884615  0.821429  0.851852  0.941406\n",
      "3                  SVM  0.850000   0.880000  0.785714  0.830189  0.954241\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "for name, model in models.items():\n",
    "    pipe = Pipeline([(\"scaler\", StandardScaler()), (\"clf\", model)])\n",
    "    pipe.fit(X_train, y_train)\n",
    "    y_pred = pipe.predict(X_test)\n",
    "    y_proba = pipe.predict_proba(X_test)[:,1]\n",
    "    results.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "        \"Precision\": precision_score(y_test, y_pred),\n",
    "        \"Recall\": recall_score(y_test, y_pred),\n",
    "        \"F1\": f1_score(y_test, y_pred),\n",
    "        \"ROC_AUC\": roc_auc_score(y_test, y_proba)\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce6ce5a-c0bd-4ab5-9cc0-b08501d83f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"../results/supervised_results.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c48f4d1d-cb29-482d-9e20-94e7103fefcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved evaluation metrics to ../results/evaluation_metrics.txt\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.read_csv(\"../results/supervised_results.csv\")\n",
    "\n",
    "with open(\"../results/evaluation_metrics.txt\", \"w\") as f:\n",
    "    f.write(\"Supervised Learning Evaluation Metrics\\n\")\n",
    "    f.write(\"=\"*50 + \"\\n\\n\")\n",
    "    for i, row in results_df.iterrows():\n",
    "        f.write(f\"Model: {row['Model']}\\n\")\n",
    "        f.write(f\"Accuracy: {row['Accuracy']:.4f}\\n\")\n",
    "        f.write(f\"Precision: {row['Precision']:.4f}\\n\")\n",
    "        f.write(f\"Recall: {row['Recall']:.4f}\\n\")\n",
    "        f.write(f\"F1 Score: {row['F1']:.4f}\\n\")\n",
    "        f.write(f\"ROC AUC: {row['ROC_AUC']:.4f}\\n\")\n",
    "        f.write(\"-\"*50 + \"\\n\")\n",
    "\n",
    "print(\"Saved evaluation metrics to ../results/evaluation_metrics.txt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57833f73-04f9-4160-8edb-12def706daf7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
